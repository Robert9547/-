{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 說明:\n",
    "# 本ipynb爬取所有yahoo新聞的使用者留言，並計算使用者正負向分數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import import_ipynb\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from chinese_tokenizer import chineseWordTokenizer\n",
    "from emotionalDictionaryExtractor import get_positive_emotional_tokens\n",
    "from emotionalDictionaryExtractor import get_negative_emotional_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#全域變數\n",
    "good_word = get_positive_emotional_tokens() #所有正向情緒詞彙\n",
    "bad_word = get_negative_emotional_tokens()  #所有負向情緒詞彙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入新聞JSON檔的資料\n",
    "with open('newsDatas.json') as f:\n",
    "    all_news = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#爬取某個yahoo新聞的使用者留言\n",
    "def getMessage(newUrl: str):\n",
    "    try:\n",
    "        # 要爬取的Yahoo新聞網址url\n",
    "        #  = 'https://tw.news.yahoo.com/新聞台撤照案-中天勝訴-北高行撤銷ncc違法處分-201000270.html'\n",
    "        # url = 'https://tw.news.yahoo.com/朱立倫48小時2度會郭台銘-達成徵召侯友宜決定-130649010.html'\n",
    "        # 發送HTTP GET請求到該網址\n",
    "        response = requests.get(newUrl)\n",
    "\n",
    "        # 建立BeautifulSoup物件\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # spaceId\n",
    "        id_soup = soup.find_all(\n",
    "            'script', src=re.compile(\"https://s.yimg.com/aaq/c/\"))\n",
    "        idString = id_soup[0].find_next()\n",
    "        id = str(idString).split(\"'\")[1]\n",
    "        # print(\"spaceId:\"+id)\n",
    "\n",
    "        # context\n",
    "        context_soup = soup.find('meta', property=\"al:ios:url\")\n",
    "        context = context_soup['content'].split('/')[4]\n",
    "        # print(\"context:\"+context)\n",
    "\n",
    "        # messageSum\n",
    "\n",
    "        urlMessage = \"https://tw.news.yahoo.com/_td-news/api/resource/canvass.getPresence_ns;apiVersion=v1;context=\"+context + \\\n",
    "            \";lang=zh-Hant-TW;messageIds=;namespace=yahoo_content;oauthConsumerKey=frontpage.oauth.canvassKey;oauthConsumerSecret=frontpage.oauth.canvassSecret;region=TW?\"\n",
    "        payloadMessage = {}\n",
    "        headersMessage = {}\n",
    "        responseMessage = requests.request(\n",
    "            \"GET\", urlMessage, headers=headersMessage, data=payloadMessage)\n",
    "        messageSum = str(responseMessage.json()['messageCount'])\n",
    "        # print(messageSum)\n",
    "\n",
    "        # crawl\n",
    "        url = \"https://tw.news.yahoo.com/_td-news/api/resource/canvass.getMessageListForContext_ns;apiVersion=v1;context=\"+context+\";count=\"+\"100\" + \\\n",
    "            \";index=null;lang=zh-Hant-TW;namespace=yahoo_content;oauthConsumerKey=frontpage.oauth.canvassKey;oauthConsumerSecret=frontpage.oauth.canvassSecret;rankingProfile=;region=TW;sortBy=popular;spaceId=\"+id+\";\"\n",
    "\n",
    "        payload = {}\n",
    "        headers = {}\n",
    "\n",
    "        response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "        message = []\n",
    "        for i in response.json()['canvassMessages']:\n",
    "            message_token = chineseWordTokenizer(i['details']['userText'])\n",
    "            message.append(message_token)\n",
    "\n",
    "        return message\n",
    "    except Exception as e:\n",
    "        print(\"讀取某新聞的留言時發生錯誤: {}\".format(e))\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#從已載入的JSON資料中找到某主題內所有yahoo新聞的媒體、標題，留言，並把留言切成tokens\n",
    "def get_allCommentsOnSpecificTopicFromYahoo(topicNum:int):\n",
    "    try:\n",
    "        allCommentsOnSpecificTopicFromYahoo = {}\n",
    "        for topic in all_news:\n",
    "            if topic['topicNum'] == topicNum: # 找到主題\n",
    "                for news in topic['allRelatedNews']:\n",
    "                    if news['media'] == \"Yahoo奇摩新聞\":\n",
    "                        message_tokens = getMessage(news['url']) #from kuan\n",
    "                        allCommentsOnSpecificTopicFromYahoo[news['docID']]  = {'media':news['media'],'title':news['title'],'message_tokens':message_tokens}           \n",
    "                break\n",
    "        return allCommentsOnSpecificTopicFromYahoo\n",
    "    except Exception as e:\n",
    "        print(\"讀取所有新聞的留言時發生錯誤: {}\".format(e))\n",
    "        return dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#計算某詞的tf\n",
    "def calculate_tf(word_list, comment):\n",
    "    count = sum([1 for w in comment if w in word_list])\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算某篇新聞的所有使用者留言的正負向分數\n",
    "def calculate_comment_score_in_a_news(docID,allCommentsOnSpecificTopic,txt_file):\n",
    "    total_score = 0\n",
    "    scores = []\n",
    "    comments = []\n",
    "    comments = allCommentsOnSpecificTopic[docID]['message_tokens']\n",
    "    if len(comments) > 0:\n",
    "        for comment in comments:\n",
    "            one_comment_score = calculate_tf(good_word, comment) - calculate_tf(bad_word, comment)\n",
    "            scores.append(one_comment_score)\n",
    "            total_score += one_comment_score\n",
    "        txt_file.write(f\"新聞{docID}\\n\")\n",
    "        txt_file.write(f\"標題： {allCommentsOnSpecificTopic[docID]['title']}\\n\")\n",
    "        txt_file.write(f\"留言人數： {len(comments)}\\n\")\n",
    "        txt_file.write(f\"個別留言分數： {scores}\\n\")\n",
    "        txt_file.write(f\"平均留言分數： {round(total_score/len(comments), 3)}\\n\")\n",
    "        txt_file.write(\"-----\"*10 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#計算某主題內所有新聞的所有使用者留言的正負向分數\n",
    "def calculate_comment_score_on_specific_topic(topicNum):\n",
    "    allCommentsOnSpecificTopic = get_allCommentsOnSpecificTopicFromYahoo(topicNum)\n",
    "    if len(allCommentsOnSpecificTopic) > 0:\n",
    "        txt_file = open(\"0525以前的使用者評分結果(by ckiptagger).txt\", \"a\")\n",
    "        txt_file.write(f\"主題 {topicNum}\\n\")\n",
    "        txt_file.write(\"=\"*10 + \"\\n\")\n",
    "        for docID in allCommentsOnSpecificTopic.keys():\n",
    "            calculate_comment_score_in_a_news(docID,allCommentsOnSpecificTopic,txt_file)\n",
    "        txt_file.write(\"\\n\\n\\n\")\n",
    "        txt_file.close() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #測試 \n",
    "# 計算所有主題的所有文章的留言分數\n",
    "for i in range(418,420):\n",
    "    calculate_comment_score_on_specific_topic(i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
